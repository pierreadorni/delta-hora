{
    "model": {
        "name": "meta-llama/Llama-2-7b-hf",
        "cache_dir": "./models",
        "quantization": {
            "load_in_4bit": true,
            "bnb_4bit_compute_dtype": "float16",
            "bnb_4bit_quant_type": "nf4",
            "bnb_4bit_use_double_quant": true
        }
    },
    "lora": {
        "r": 16,
        "lora_alpha": 32,
        "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "lora_dropout": 0.1,
        "bias": "none",
        "task_type": "CAUSAL_LM"
    },
    "training": {
        "output_dir": "./results",
        "num_train_epochs": 3,
        "per_device_train_batch_size": 4,
        "per_device_eval_batch_size": 4,
        "gradient_accumulation_steps": 4,
        "learning_rate": 2e-4,
        "max_grad_norm": 1.0,
        "weight_decay": 0.01,
        "warmup_ratio": 0.1,
        "lr_scheduler_type": "cosine",
        "logging_steps": 10,
        "evaluation_strategy": "steps",
        "eval_steps": 100,
        "save_strategy": "steps",
        "save_steps": 500,
        "save_total_limit": 2,
        "load_best_model_at_end": true,
        "metric_for_best_model": "eval_loss",
        "greater_is_better": false,
        "dataloader_num_workers": 4,
        "remove_unused_columns": false,
        "optim": "paged_adamw_32bit"
    },
    "data": {
        "dataset_name": "ChilleD/MAWPS",
        "max_length": 512,
        "train_split": "train",
        "test_split": "test",
        "instruction_template": "Below is a math word problem. Solve it step by step.\n\n### Problem:\n{problem}\n\n### Solution:\n{solution}",
        "response_template": "### Solution:\n"
    },
    "logging": {
        "use_wandb": true,
        "wandb_project": "llama2-mawps-finetuning",
        "wandb_entity": null,
        "log_level": "INFO"
    },
    "inference": {
        "max_new_tokens": 256,
        "temperature": 0.1,
        "do_sample": true,
        "top_p": 0.9,
        "repetition_penalty": 1.1
    }
}
