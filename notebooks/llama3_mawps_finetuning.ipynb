{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f34e75a",
   "metadata": {},
   "source": [
    "# Llama3 7B Fine-tuning on MAWPS Dataset with LoRA\n",
    "\n",
    "This notebook demonstrates how to fine-tune Llama3 7B on the MAWPS (Math Word Problems) dataset using LoRA (Low-Rank Adaptation) for efficient parameter training.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Model**: Llama3 7B (or compatible model)\n",
    "- **Dataset**: MAWPS - Math word problems dataset\n",
    "- **Method**: LoRA fine-tuning for efficient training\n",
    "- **Goal**: Improve model's ability to solve math word problems\n",
    "\n",
    "## Features\n",
    "\n",
    "- Efficient training with LoRA (Low-Rank Adaptation)\n",
    "- 4-bit quantization for memory efficiency\n",
    "- Custom evaluation metrics for math problems\n",
    "- Modular code structure for easy experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e528d",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies\n",
    "\n",
    "First, let's install all the necessary libraries for fine-tuning Llama3 with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f72bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.12.7)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/adorni/Documents/delta-hora/venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Run this cell if packages are not already installed\n",
    "\n",
    "# !pip install torch>=2.0.0\n",
    "# !pip install transformers>=4.35.0\n",
    "# !pip install datasets>=2.14.0\n",
    "# !pip install peft>=0.6.0\n",
    "# !pip install accelerate>=0.24.0\n",
    "# !pip install bitsandbytes>=0.41.0\n",
    "# !pip install trl>=0.7.0\n",
    "# !pip install wandb>=0.15.0\n",
    "# !pip install scikit-learn>=1.3.0\n",
    "\n",
    "print(\"Dependencies installation commands are ready!\")\n",
    "print(\"Uncomment and run the pip install commands if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd00bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adorni/Documents/delta-hora/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "Transformers version: 4.54.1\n",
      "CUDA available: True\n",
      "GPU: NVIDIA RTX 500 Ada Generation Laptop GPU\n",
      "GPU Memory: 4.1 GB\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728a1c7",
   "metadata": {},
   "source": [
    "## 2. Load and Explore MAWPS Dataset\n",
    "\n",
    "The MAWPS (Math Word Problems) dataset contains grade school math word problems. Let's load and explore the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e47c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MAWPS dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1772/1772 [00:00<00:00, 31752.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Question', 'Equation', 'Answer', 'Numbers'],\n",
      "        num_rows: 1772\n",
      "    })\n",
      "})\n",
      "\n",
      "train split:\n",
      "  Number of examples: 1772\n",
      "  Features: ['Question', 'Equation', 'Answer', 'Numbers']\n",
      "\n",
      "Sample examples from training set:\n",
      "\n",
      "Example 1:\n",
      "  Question: Mary is baking a cake . The recipe wants N_00 cups of flour . She already put in N_01 cups . How many cups does she need to add ?\n",
      "  Equation: N_00 - N_01\n",
      "  Answer: 6.0\n",
      "  Numbers: 8.0 2.0\n",
      "\n",
      "Example 2:\n",
      "  Question: There are N_00 erasers and N_01 scissors in the drawer . Jason placed N_02 erasers in the drawer . How many erasers are now there in total ?\n",
      "  Equation: N_00 + N_02\n",
      "  Answer: 270.0\n",
      "  Numbers: 139.0 118.0 131.0\n",
      "\n",
      "Example 3:\n",
      "  Question: One pencil weighs N_00 grams . How much do N_01 pencils weigh ?\n",
      "  Equation: N_00 * N_01\n",
      "  Answer: 141.5\n",
      "  Numbers: 28.3 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MAWPS dataset\n",
    "print(\"Loading MAWPS dataset...\")\n",
    "dataset = load_dataset(\"mwpt5/MAWPS\")\n",
    "\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Explore the dataset\n",
    "for split_name, split_data in dataset.items():\n",
    "    print(f\"\\n{split_name} split:\")\n",
    "    print(f\"  Number of examples: {len(split_data)}\")\n",
    "    print(f\"  Features: {list(split_data.features.keys())}\")\n",
    "\n",
    "# Look at sample examples\n",
    "print(\"\\nSample examples from training set:\")\n",
    "for i in range(3):\n",
    "    example = dataset['train'][i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    for key, value in example.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6fbd5",
   "metadata": {},
   "source": [
    "## 3. Load Llama3 7B Model with LoRA Configuration\n",
    "\n",
    "Now let's load the Llama3 7B model (or a compatible model) and configure LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20863585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen3-0.6B\n",
      "Max length: 512\n",
      "LoRA configuration: {'r': 16, 'lora_alpha': 32, 'target_modules': ['attn.c_attn', 'attn.c_proj', 'mlp.c_fc1', 'mlp.c_fc2'], 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM'}\n",
      "\n",
      "Note: Using DialoGPT as an open-access alternative.\n",
      "To use Llama models:\n",
      "1. Get approval at: https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
      "2. Login with: huggingface-cli login\n",
      "3. Then change MODEL_NAME back to 'meta-llama/Llama-2-7b-hf'\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "# Using an open-access model that doesn't require authentication\n",
    "# Options: microsoft/DialoGPT-medium, microsoft/DialoGPT-large, or any other open model\n",
    "# MODEL_NAME = \"microsoft/DialoGPT-medium\"  # Open-access alternative\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1b-Instruct\"  # Requires HF authentication and approval\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "# MODEL_NAME = \"huggyllama/llama-7b\"       # Another option if you have access\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# LoRA configuration - adjusted for Qwen3\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,                    # Rank\n",
    "    \"lora_alpha\": 32,          # Alpha parameter for LoRA scaling\n",
    "    \"target_modules\": [        # Target modules for LoRA (adjusted for Qwen3)\n",
    "\"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    \"lora_dropout\": 0.1,       # Dropout probability for LoRA layers\n",
    "    \"bias\": \"none\",            # Bias type\n",
    "    \"task_type\": \"CAUSAL_LM\"   # Task type\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Max length: {MAX_LENGTH}\")\n",
    "print(f\"LoRA configuration: {LORA_CONFIG}\")\n",
    "print(\"\\nNote: Using DialoGPT as an open-access alternative.\")\n",
    "print(\"To use Llama models:\")\n",
    "print(\"1. Get approval at: https://huggingface.co/meta-llama/Llama-2-7b-hf\")\n",
    "print(\"2. Login with: huggingface-cli login\")\n",
    "print(\"3. Then change MODEL_NAME back to 'meta-llama/Llama-2-7b-hf'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fe389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Login to Hugging Face (only needed for gated models like Llama)\n",
    "# Uncomment and run this cell if you want to use Llama models\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "# Or alternatively, set your token directly:\n",
    "# import os\n",
    "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_token_here\"\n",
    "\n",
    "print(\"Authentication cell ready (currently commented out)\")\n",
    "print(\"Uncomment lines above if you need to authenticate for gated models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22ff9e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Tokenizer loaded. Vocab size: 151669\n",
      "Loading base model...\n",
      "Tokenizer loaded. Vocab size: 151669\n",
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"Base model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d705a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LoRA...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules {'mlp.c_fc2', 'mlp.c_fc1', 'attn.c_proj', 'attn.c_attn'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m lora_config = LoraConfig(**LORA_CONFIG)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Apply LoRA to the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Print trainable parameters\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_trainable_parameters\u001b[39m(model):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/delta-hora/venv/lib/python3.12/site-packages/peft/mapping_func.py:125\u001b[39m, in \u001b[36mget_peft_model\u001b[39m\u001b[34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config.is_prompt_learning:\n\u001b[32m    124\u001b[39m     peft_config = _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/delta-hora/venv/lib/python3.12/site-packages/peft/peft_model.py:1810\u001b[39m, in \u001b[36mPeftModelForCausalLM.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, **kwargs)\u001b[39m\n\u001b[32m   1807\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1808\u001b[39m     \u001b[38;5;28mself\u001b[39m, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, **kwargs\n\u001b[32m   1809\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1810\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1811\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/delta-hora/venv/lib/python3.12/site-packages/peft/peft_model.py:130\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    128\u001b[39m     ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m         \u001b[38;5;28mself\u001b[39m.base_model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, \u001b[33m\"\u001b[39m\u001b[33m_cast_adapter_dtype\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model._cast_adapter_dtype(\n\u001b[32m    134\u001b[39m         adapter_name=adapter_name, autocast_adapter_dtype=autocast_adapter_dtype\n\u001b[32m    135\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/delta-hora/venv/lib/python3.12/site-packages/peft/tuners/lora/model.py:143\u001b[39m, in \u001b[36mLoraModel.__init__\u001b[39m\u001b[34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name, low_cpu_mem_usage: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/delta-hora/venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:203\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_injection_hook(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name], adapter_name)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config != PeftType.XLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/delta-hora/venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:550\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    549\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m You also specified \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config.layers_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# Some modules did not match and some matched but were excluded\u001b[39;00m\n\u001b[32m    553\u001b[39m     error_msg = (\n\u001b[32m    554\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo modules were targeted for adaptation. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    555\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis might be caused by a combination of mismatched target modules and excluded modules. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    556\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check your `target_modules` and `exclude_modules` configuration. You may also have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33monly targeted modules that are marked to be saved (`modules_to_save`).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Target modules {'mlp.c_fc2', 'mlp.c_fc1', 'attn.c_proj', 'attn.c_attn'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "# Setup LoRA\n",
    "print(\"Setting up LoRA...\")\n",
    "lora_config = LoraConfig(**LORA_CONFIG)\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable params: {trainable_params:,} || \"\n",
    "        f\"All params: {all_param:,} || \"\n",
    "        f\"Trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "print(\"LoRA setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b46ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc23b4",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset for Training\n",
    "\n",
    "Now let's preprocess the MAWPS dataset by formatting it into instruction-response pairs suitable for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instruction template\n",
    "INSTRUCTION_TEMPLATE = \"\"\"Below is a math word problem. Solve it step by step.\n",
    "\n",
    "### Problem:\n",
    "{problem}\n",
    "\n",
    "### Solution:\n",
    "{solution}\"\"\"\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format a single example using the instruction template.\"\"\"\n",
    "    # Handle different possible field names in MAWPS dataset\n",
    "    problem = example.get('Question', example.get('question', example.get('Problem', '')))\n",
    "    solution = example.get('Answer', example.get('answer', example.get('Solution', '')))\n",
    "    \n",
    "    # Ensure solution is a string\n",
    "    if isinstance(solution, (int, float)):\n",
    "        solution = str(solution)\n",
    "    \n",
    "    formatted_text = INSTRUCTION_TEMPLATE.format(problem=problem, solution=solution)\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Test formatting with a sample\n",
    "sample_example = dataset['train'][0]\n",
    "formatted_sample = format_instruction(sample_example)\n",
    "print(\"Sample formatted example:\")\n",
    "print(formatted_sample[\"text\"])\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize examples for training.\"\"\"\n",
    "    # Format all examples\n",
    "    formatted_texts = [format_instruction(example)[\"text\"] for example in examples]\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        formatted_texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"Preprocessing datasets...\")\n",
    "\n",
    "# For demonstration, we'll use a subset of the data\n",
    "# In practice, you might want to use the full dataset\n",
    "train_dataset = dataset['train'].select(range(1000))  # Use first 1000 examples\n",
    "eval_dataset = dataset['test'].select(range(200))     # Use first 200 examples for eval\n",
    "\n",
    "# Apply formatting and tokenization\n",
    "def preprocess_batch(examples):\n",
    "    \"\"\"Preprocess a batch of examples.\"\"\"\n",
    "    formatted_examples = []\n",
    "    for i in range(len(examples[list(examples.keys())[0]])):\n",
    "        example = {key: examples[key][i] for key in examples.keys()}\n",
    "        formatted_examples.append(example)\n",
    "    \n",
    "    return tokenize_function(formatted_examples)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train dataset\"\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    desc=\"Tokenizing eval dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "print(f\"Sample tokenized length: {len(train_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46021415",
   "metadata": {},
   "source": [
    "## 5. Configure Training Arguments\n",
    "\n",
    "Set up the training configuration with appropriate hyperparameters for LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b729596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_llama_mawps\",\n",
    "    num_train_epochs=2,                    # Number of training epochs\n",
    "    per_device_train_batch_size=4,         # Batch size per device during training\n",
    "    per_device_eval_batch_size=4,          # Batch size for evaluation\n",
    "    gradient_accumulation_steps=4,         # Steps to accumulate gradients\n",
    "    learning_rate=2e-4,                    # Learning rate\n",
    "    max_grad_norm=1.0,                     # Max gradient norm for clipping\n",
    "    weight_decay=0.01,                     # Weight decay\n",
    "    warmup_ratio=0.1,                      # Warmup ratio\n",
    "    lr_scheduler_type=\"cosine\",            # Learning rate scheduler\n",
    "    logging_steps=10,                      # Log every N steps\n",
    "    evaluation_strategy=\"steps\",           # Evaluation strategy\n",
    "    eval_steps=100,                        # Evaluate every N steps\n",
    "    save_strategy=\"steps\",                 # Save strategy\n",
    "    save_steps=200,                        # Save every N steps\n",
    "    save_total_limit=2,                    # Maximum number of checkpoints to keep\n",
    "    load_best_model_at_end=True,          # Load best model at the end\n",
    "    metric_for_best_model=\"eval_loss\",     # Metric to use for best model\n",
    "    greater_is_better=False,               # Whether metric should be maximized\n",
    "    \n",
    "    # Performance optimizations\n",
    "    fp16=True,                             # Use mixed precision training\n",
    "    dataloader_pin_memory=False,           # Don't pin memory (can cause issues with some setups)\n",
    "    group_by_length=True,                  # Group sequences by length for efficiency\n",
    "    optim=\"paged_adamw_32bit\",            # Optimizer\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=[],                          # Don't report to wandb for this demo\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Output directory: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e352b",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer and Start Fine-tuning\n",
    "\n",
    "Create the trainer and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal language modeling, not masked language modeling\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Start training\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training steps: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869048c1",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance\n",
    "\n",
    "Evaluate the fine-tuned model and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7cb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in eval_result.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Custom evaluation function for math problems\n",
    "def extract_number(text):\n",
    "    \"\"\"Extract numerical answer from generated text.\"\"\"\n",
    "    # Look for numbers at the end of the text\n",
    "    numbers = re.findall(r'[-+]?\\d*\\.?\\d+', text)\n",
    "    if numbers:\n",
    "        try:\n",
    "            return float(numbers[-1])\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def evaluate_math_problems(model, tokenizer, eval_examples, num_samples=10):\n",
    "    \"\"\"Evaluate model on math problems.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(min(num_samples, len(eval_examples))):\n",
    "        example = eval_examples[i]\n",
    "        \n",
    "        # Get the original problem\n",
    "        problem = example.get('Question', example.get('question', example.get('Problem', '')))\n",
    "        expected_answer = example.get('Answer', example.get('answer', example.get('Solution', '')))\n",
    "        \n",
    "        # Generate solution\n",
    "        prompt = f\"Below is a math word problem. Solve it step by step.\\n\\n### Problem:\\n{problem}\\n\\n### Solution:\\n\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=128,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract solution\n",
    "        if \"### Solution:\" in generated_text:\n",
    "            solution = generated_text.split(\"### Solution:\")[-1].strip()\n",
    "        else:\n",
    "            solution = generated_text.strip()\n",
    "        \n",
    "        # Extract numerical answers\n",
    "        predicted_num = extract_number(solution)\n",
    "        expected_num = extract_number(str(expected_answer))\n",
    "        \n",
    "        if predicted_num is not None and expected_num is not None:\n",
    "            if abs(predicted_num - expected_num) < 1e-6:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Problem: {problem}\")\n",
    "            print(f\"Expected: {expected_answer}\")\n",
    "            print(f\"Predicted: {solution}\")\n",
    "            print(f\"Correct: {abs(predicted_num - expected_num) < 1e-6}\")\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\nAccuracy on {total} problems: {accuracy:.2%}\")\n",
    "    return accuracy\n",
    "\n",
    "# Run custom evaluation\n",
    "original_eval_data = dataset['test'].select(range(10))  # Use original data for evaluation\n",
    "accuracy = evaluate_math_problems(model, tokenizer, original_eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7caa34",
   "metadata": {},
   "source": [
    "## 8. Save and Load Fine-tuned Model\n",
    "\n",
    "Save the LoRA adapter weights and demonstrate how to load them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed797ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "model_save_path = \"./llama_mawps_lora\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "# Demonstrate how to load the model later\n",
    "print(\"\\nDemonstrating model loading...\")\n",
    "\n",
    "# For loading, you would:\n",
    "# 1. Load the base model\n",
    "# 2. Load the LoRA adapter\n",
    "\n",
    "# Note: In a new session, you would do this:\n",
    "\"\"\"\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "\n",
    "# Load tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Model loading instructions saved in comments above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130ff50",
   "metadata": {},
   "source": [
    "## 9. Test Model with Custom Examples\n",
    "\n",
    "Test the fine-tuned model with custom math word problems to see the improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom math problems\n",
    "custom_problems = [\n",
    "    \"Sarah has 15 apples. She gives 7 apples to her friend Tom and 3 apples to her sister. How many apples does Sarah have left?\",\n",
    "    \"A school has 450 students. If 180 students are boys, how many students are girls?\",\n",
    "    \"Mike bought 3 packages of pencils. Each package contains 12 pencils. How many pencils did Mike buy in total?\",\n",
    "    \"A rectangle has a length of 8 meters and a width of 5 meters. What is the area of the rectangle?\"\n",
    "]\n",
    "\n",
    "def test_model(model, tokenizer, problem):\n",
    "    \"\"\"Test the model with a single problem.\"\"\"\n",
    "    prompt = f\"Below is a math word problem. Solve it step by step.\\n\\n### Problem:\\n{problem}\\n\\n### Solution:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract solution\n",
    "    if \"### Solution:\" in generated_text:\n",
    "        solution = generated_text.split(\"### Solution:\")[-1].strip()\n",
    "    else:\n",
    "        solution = generated_text.strip()\n",
    "    \n",
    "    return solution\n",
    "\n",
    "print(\"Testing fine-tuned model with custom problems:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, problem in enumerate(custom_problems, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Problem: {problem}\")\n",
    "    \n",
    "    solution = test_model(model, tokenizer, problem)\n",
    "    print(f\"Solution: {solution}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nTesting completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506428e",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully fine-tuned Llama3 7B on the MAWPS dataset using LoRA.\n",
    "\n",
    "### What we accomplished:\n",
    "- ✅ Loaded and explored the MAWPS dataset\n",
    "- ✅ Set up Llama3 7B with 4-bit quantization\n",
    "- ✅ Configured LoRA for efficient fine-tuning\n",
    "- ✅ Preprocessed the dataset into instruction-response format\n",
    "- ✅ Fine-tuned the model with only ~0.5% trainable parameters\n",
    "- ✅ Evaluated the model performance\n",
    "- ✅ Saved the LoRA adapter for future use\n",
    "\n",
    "### Next Steps:\n",
    "1. **Experiment with hyperparameters**: Try different LoRA ranks, learning rates, or batch sizes\n",
    "2. **Use the full dataset**: We used a subset for demonstration - use the full dataset for better results\n",
    "3. **Add better evaluation metrics**: Implement more sophisticated math problem evaluation\n",
    "4. **Deploy the model**: Create an API or web interface for the fine-tuned model\n",
    "5. **Compare with base model**: Evaluate the base model on the same problems to see improvement\n",
    "\n",
    "### Modifying the Project:\n",
    "- **Change the model**: Replace `MODEL_NAME` with any compatible model (Llama2, CodeLlama, etc.)\n",
    "- **Adjust LoRA config**: Modify `LORA_CONFIG` to experiment with different adapter settings\n",
    "- **Change the dataset**: Replace MAWPS with any other instruction-following dataset\n",
    "- **Modify the prompt template**: Customize `INSTRUCTION_TEMPLATE` for different tasks\n",
    "\n",
    "The project structure is modular and easily extensible for your specific needs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
